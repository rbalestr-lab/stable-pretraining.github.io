{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example demonstrates how to use stable-SSL to train a supervised model on CIFAR10 with class imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import hydra\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom omegaconf import DictConfig\nfrom torchvision import transforms\n\nimport stable_pretraining as spt\nfrom stable_pretraining.supervised import Supervised\n\n\nclass MyCustomSupervised(Supervised):\n    \"\"\"Custom supervised example model for an imbalanced dataset.\"\"\"\n\n    def initialize_train_loader(self):\n        transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ]\n        )\n        trainset = torchvision.datasets.CIFAR10(\n            root=self.config.root, train=True, download=True, transform=transform\n        )\n        distribution = np.exp(np.linspace(0, self.config.distribution, 10))\n        distribution /= np.sum(distribution)\n        trainset = spt.base.resample_classes(trainset, distribution)\n        trainloader = torch.utils.data.DataLoader(\n            trainset,\n            batch_size=self.config.optim.batch_size,\n            shuffle=True,\n            num_workers=2,\n            drop_last=True,\n        )\n        return trainloader\n\n    def initialize_test_loader(self):\n        transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ]\n        )\n        testset = torchvision.datasets.CIFAR10(\n            root=self.config.root, train=False, download=True, transform=transform\n        )\n        testloader = torch.utils.data.DataLoader(\n            testset, batch_size=self.config.optim.batch_size, num_workers=2\n        )\n        return testloader\n\n    def initialize_modules(self):\n        self.model = spt.utils.nn.resnet9()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def compute_loss(self):\n        \"\"\"The compute_loss method is called during training on each mini-batch.\n\n        stable-SSL automatically stores the output of the data loader as `self.data`,\n        which you can access directly within this function.\n        \"\"\"\n        preds = self.forward(self.data[0])\n        print(self.data[1][:4])\n        self.log(\n            {\"train/step/acc1\": self.metrics[\"train/step/acc1\"](preds, self.data[1])},\n            commit=False,\n        )\n        return F.cross_entropy(preds, self.data[1])\n\n\n@hydra.main(version_base=None)\ndef main(cfg: DictConfig):\n    args = spt.get_args(cfg)\n\n    print(\"--- Arguments ---\")\n    print(args)\n\n    # while we provide a lot of config parameters (e.g. `optim.batch_size`), you can\n    # also pass arguments directly when calling your model, they will be logged and\n    #  accessible from within the model as `self.config.root` (in this example)\n    trainer = MyCustomSupervised(args, root=\"~/data\")\n    trainer()\n\n\ndef visualization():\n    import matplotlib.pyplot as plt\n    import seaborn\n    from matplotlib import colormaps\n\n    seaborn.set(font_scale=2)\n\n    cmap = colormaps.get_cmap(\"cool\")\n\n    configs, values = spt.reader.jsonl_project(\"experiment_llm\")\n    distris = {j: i for i, j in enumerate(np.unique(configs[\"distribution\"]))}\n    print(distris)\n    fig, axs = plt.subplots(1, 1, sharey=\"all\", sharex=\"all\", figsize=(10, 7))\n\n    for (_, c), v in zip(configs.iterrows(), values):\n        if c[\"distribution\"] > 0.01:\n            continue\n        axs.plot(\n            v[-1][\"eval/epoch/acc1_by_class\"],\n            c=cmap(np.sqrt(np.sqrt(c[\"optim.weight_decay\"] / 10))),\n            linewidth=3,\n        )\n        print(\n            \"(\",\n            c[\"optim.weight_decay\"],\n            \",\",\n            np.round(100 * np.array(v[-1][\"eval/epoch/acc1_by_class\"]), 2),\n            \")\",\n        )\n\n    plt.ylabel(\"test accuracy\")\n    plt.xlabel(\"class index\")\n    plt.tight_layout()\n    plt.savefig(\"imbalance_classification.png\")\n    plt.close()\n\n\nif __name__ == \"__main__\":\n    main()\n    visualization()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}