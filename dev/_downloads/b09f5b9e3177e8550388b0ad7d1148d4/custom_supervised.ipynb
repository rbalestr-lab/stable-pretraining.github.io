{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example demonstrates how to create a custom supervised model using the `stable_ssl` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import hydra\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom omegaconf import DictConfig\nfrom torchvision import transforms\n\nimport stable_ssl\nfrom stable_ssl.supervised import Supervised\n\n\nclass MyCustomSupervised(Supervised):\n    \"\"\"Custom supervised example model.\"\"\"\n\n    def initialize_train_loader(self):\n        transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ]\n        )\n        trainset = torchvision.datasets.CIFAR10(\n            root=self.config.root, train=True, download=True, transform=transform\n        )\n        trainloader = torch.utils.data.DataLoader(\n            trainset,\n            batch_size=self.config.optim.batch_size,\n            shuffle=True,\n            num_workers=2,\n            drop_last=True,\n        )\n        return trainloader\n\n    def initialize_test_loader(self):\n        transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ]\n        )\n        testset = torchvision.datasets.CIFAR10(\n            root=self.config.root, train=False, download=True, transform=transform\n        )\n        testloader = torch.utils.data.DataLoader(\n            testset,\n            batch_size=self.config.optim.batch_size,\n            shuffle=False,\n            num_workers=2,\n        )\n        return testloader\n\n    def compute_loss(self):\n        \"\"\"The compute_loss method is called during training on each mini-batch.\n\n        stable-SSL automatically stores the output of the data loader as `self.data`,\n        which you can access directly within this function.\n        \"\"\"\n        preds = self.forward(self.data[0])\n        return F.cross_entropy(preds, self.data[1])\n\n\n@hydra.main()\ndef main(cfg: DictConfig):\n    args = stable_ssl.get_args(cfg)\n\n    print(\"--- Arguments ---\")\n    print(args)\n\n    # while we provide a lot of config parameters (e.g. `optim.batch_size`), you can\n    # also pass arguments directly when calling your model, they will be logged and\n    # accessible from within the model as `self.config.root` (in this example)\n    trainer = MyCustomSupervised(args, root=\"~/data\")\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}