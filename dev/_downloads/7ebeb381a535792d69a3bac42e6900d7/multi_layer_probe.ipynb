{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi-layer probe for vision models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nfrom typing import Dict, List, Tuple\n\nimport hydra\nimport lightning as pl\nimport torch\nimport torchmetrics\nimport torchvision\nfrom datasets import load_dataset\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import WandbLogger  # type: ignore\nfrom omegaconf import DictConfig\nfrom torch import nn\nfrom transformers import (\n    AutoImageProcessor,\n    AutoModel,\n    AutoModelForZeroShotImageClassification,\n    AutoProcessor,\n)\n\nimport stable_pretraining as spt\nfrom stable_pretraining.data import transforms\n\n# -----------------------------\n# Model registry\n# -----------------------------\nMODEL_ZOO = {\n    \"DINOv2\": {\n        \"processor_cls\": AutoImageProcessor,\n        \"processor_name\": \"facebook/dinov2-base\",\n        \"model_cls\": AutoModel,\n        \"model_name\": \"facebook/dinov2-base\",\n        \"pooling\": \"cls\",\n        \"probe_skip\": 1,\n    },\n    \"DINOv3\": {\n        \"processor_cls\": AutoImageProcessor,\n        \"processor_name\": \"facebook/dinov3-vitb16-pretrain-lvd1689m\",\n        \"model_cls\": AutoModel,\n        \"model_name\": \"facebook/dinov3-vitb16-pretrain-lvd1689m\",\n        \"pooling\": \"cls\",\n        \"probe_skip\": 1,\n    },\n    \"MetaCLIP\": {\n        \"processor_cls\": AutoProcessor,\n        \"processor_name\": \"facebook/metaclip-b16-400m\",\n        \"model_cls\": AutoModelForZeroShotImageClassification,\n        \"model_name\": \"facebook/metaclip-b16-400m\",\n        \"pooling\": \"mean\",\n        \"probe_skip\": 1,\n    },\n    \"IJEPA-1k\": {\n        \"processor_cls\": AutoImageProcessor,\n        \"processor_name\": \"facebook/ijepa_vith14_1k\",\n        \"model_cls\": AutoModel,\n        \"model_name\": \"facebook/ijepa_vith14_1k\",\n        \"pooling\": \"mean\",\n        \"probe_skip\": 1,\n    },\n    \"IJEPA-22k\": {\n        \"processor_cls\": AutoImageProcessor,\n        \"processor_name\": \"facebook/ijepa_vith14_22k\",\n        \"model_cls\": AutoModel,\n        \"model_name\": \"facebook/ijepa_vith14_22k\",\n        \"pooling\": \"mean\",\n        \"probe_skip\": 1,\n    },\n}\n\n\n# -----------------------------\n# Utilities\n# -----------------------------\n\n\ndef build_datasets() -> Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    # Load the Hugging Face dataset\n    train_dataset = spt.data.HFDataset(\n        \"clane9/imagenet-100\",\n        split=\"train\",\n        transform=transforms.RGB(),\n    )\n\n    val_dataset = spt.data.HFDataset(\n        \"clane9/imagenet-100\",\n        split=\"validation\",\n        transform=transforms.RGB(),\n    )\n\n    return train_dataset, val_dataset\n\n\ndef make_collate_fn(processor):\n    def collate_fn(examples):\n        images = [ex[\"image\"] for ex in examples]\n        labels = torch.tensor([ex[\"label\"] for ex in examples], dtype=torch.long)\n        batch = processor(images=images, return_tensors=\"pt\")\n        return {\"images\": batch, \"label\": labels}\n\n    return collate_fn\n\n\ndef build_dataloaders(\n    train_dataset,\n    val_dataset,\n    processor,\n    batch_size: int = 128,\n    num_workers: int = 6,\n) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        sampler=spt.data.sampler.RepeatedRandomSampler(train_dataset),\n        batch_size=batch_size,\n        num_workers=num_workers,\n        drop_last=True,\n        pin_memory=True,\n        collate_fn=make_collate_fn(processor),\n    )\n\n    val_loader = torch.utils.data.DataLoader(\n        dataset=val_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n        collate_fn=make_collate_fn(processor),\n    )\n    return train_loader, val_loader\n\n\ndef load_backbone(model_name: str):\n    spec = MODEL_ZOO[model_name]\n    processor = spec[\"processor_cls\"].from_pretrained(spec[\"processor_name\"])  # type: ignore\n    model = spec[\"model_cls\"].from_pretrained(\n        spec[\"model_name\"], output_hidden_states=True\n    )  # type: ignore\n\n    config = model.config if \"CLIP\" not in model_name else model.config.vision_config\n    emb_dim = config.hidden_size\n    num_hidden_layers = config.num_hidden_layers\n    pooling = spec[\"pooling\"]\n    probe_skip = spec.get(\"probe_skip\", 1)\n\n    if \"CLIP\" in model_name:\n        model = model.vision_model\n\n    return model, processor, emb_dim, num_hidden_layers, pooling, probe_skip\n\n\n# -----------------------------\n# Lightning-compatible `spt.Module`\n# -----------------------------\n\n\ndef build_module(\n    model, processor, transformer_block_indices: List[int], pooling: str\n) -> spt.Module:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Define the forward used by `spt.Module`\n    def forward(self, batch: Dict, stage: str):  # noqa: ARG001 (stage provided by spt)\n        out: Dict[str, torch.Tensor] = {}\n\n        # Preprocess & move to device\n        # images = processor(batch[\"image\"], return_tensors=\"pt\")\n        images = {\n            k: v.to(device=device, non_blocking=True)\n            for k, v in batch[\"images\"].items()\n        }\n\n        outputs = self.model(**images, output_hidden_states=True)\n        hiddens = outputs[\"hidden_states\"]  # tuple: [embeddings, block1, block2, ...]\n\n        # Mean-pool tokens per layer -> (B, D)\n        for i in transformer_block_indices:\n            x = hiddens[1 + i]\n\n            if pooling == \"cls\":\n                x = x[:, 0]\n            elif pooling == \"mean\":\n                x = x.mean(dim=1)\n            else:\n                raise ValueError(f\"Unknown pooling type: {pooling}\")\n\n            out[f\"embedding_layer_{i}\"] = x.detach()\n        return out\n\n    module = spt.Module(\n        model=model,  # spt.backbone.EvalOnly(model),  # freeze eval-only backbone\n        forward=forward,\n        processor=processor,\n        optim=None,  # probes have their own optimizers\n    )\n    return module\n\n\n# -----------------------------\n# Probes\n# -----------------------------\n\n\ndef build_probes(emb_dim: int, num_classes: int, transformer_block_indices: List[int]):\n    probes = []\n    for i in transformer_block_indices:\n        probes.append(\n            spt.callbacks.OnlineProbe(\n                target=\"label\",\n                name=f\"linear_probe_block_{i}\",\n                input=f\"embedding_layer_{i}\",\n                probe=nn.Sequential(\n                    nn.BatchNorm1d(emb_dim),\n                    nn.Linear(emb_dim, num_classes),\n                ),\n                loss_fn=nn.CrossEntropyLoss(),\n                metrics={\n                    \"top1\": torchmetrics.classification.MulticlassAccuracy(num_classes),\n                    \"top5\": torchmetrics.classification.MulticlassAccuracy(\n                        num_classes, top_k=5\n                    ),\n                },\n                optimizer={\"type\": \"SGD\", \"lr\": 1e-3},\n                scheduler={\"type\": \"CosineAnnealingLR\", \"T_max\": 100},\n            )\n        )\n    return probes\n\n\n# -----------------------------\n# Main\n# -----------------------------\n\n\n@hydra.main(config_path=\"config_examples\", config_name=\"multi_probe\")\ndef main(cfg: DictConfig):\n    pl.seed_everything(cfg.seed, workers=True)\n\n    # Backbone & module\n    model, processor, emb_dim, num_layers, pooling, probe_skip = load_backbone(\n        cfg.model\n    )\n    # Most ViT-like models have 12 blocks; adapt as needed\n    transformer_block_indices = list(range(0, num_layers, probe_skip))\n    module = build_module(model, processor, transformer_block_indices, pooling)\n\n    # Data\n    train_ds, val_ds = build_datasets()\n    train_loader, val_loader = build_dataloaders(\n        train_ds,\n        val_ds,\n        processor,\n        batch_size=cfg.batch_size,\n        num_workers=cfg.num_workers,\n    )\n    data = spt.data.DataModule(train=train_loader, val=val_loader)\n\n    # Probes\n    probes = build_probes(\n        emb_dim=emb_dim,\n        num_classes=100,\n        transformer_block_indices=transformer_block_indices,\n    )\n\n    # Trainer\n    precision = \"16-mixed\" if torch.cuda.is_available() else 32\n    logger = None\n    if cfg.use_wandb and WandbLogger is not None:\n        logger = WandbLogger(project=cfg.project)\n\n    checkpoint_callback = ModelCheckpoint(filename=\"ckpt\", save_last=True)\n\n    trainer = pl.Trainer(\n        max_epochs=cfg.epochs,\n        callbacks=probes + [checkpoint_callback],\n        precision=precision,\n        logger=logger,\n        enable_checkpointing=True,\n    )\n\n    # Run\n    manager = spt.Manager(trainer=trainer, module=module, data=data)\n    manager()\n\n\nif __name__ == \"__main__\":\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}